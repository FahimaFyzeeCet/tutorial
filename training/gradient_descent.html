
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Gradient descent &#8212; Open-Source Tools &amp; Data for Music Source Separation</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/sphinx-book-theme.2d2078699c18a0efb88233928e1cf6ed.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.be0a4a0c39cd630af62a2fcf693f3f06.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Coding up model architectures" href="building_blocks.html" />
    <link rel="prev" title="Introduction" href="intro.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">Open-Source Tools & Data for Music Source Separation</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../landing.html">
   Open-Source Tools &amp; Data for Music Source Separation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro/tutorial_structure.html">
   Tutorial Structure and Setup
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intro/src_sep_101.html">
   What is Source Separation?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intro/open_src_projects.html">
   Map of Open-Source Source Separation Projects
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Basics of Source Separation
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../basics/representations.html">
   Representing Audio
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../basics/tf_and_masking.html">
   TF Representations and Masking
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../basics/phase.html">
   Phase
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../basics/evaluation.html">
   Evaluation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  First Steps
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../first_steps/nussl_intro.html">
   Introduction to nussl
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../first_steps/repetition.html">
   Putting it All Together: Repetition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../first_steps/byo_hpss.html">
   Build Your Own HPSS
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Deep Learning Approaches
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../approaches/deep/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approaches/deep/building_blocks.html">
   Building Blocks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approaches/deep/architectures.html">
   Architectures
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Data
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../data/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/datasets.html">
   Datasets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/musdb18.html">
   The MUSDB18 dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/scaper.html">
   Generating mixtures with Scaper
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Training
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Gradient descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="building_blocks.html">
   Coding up model architectures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="putting_it_all_together.html">
   Putting it all together
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Conclusions
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../conclusions/applications.html">
   Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../conclusions/concluding_remarks.html">
   Concluding Remarks
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../zzz_refs.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix/resources.html">
   Additional Resources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix/acknowledgements.html">
   Acknowledgements
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix/authors.html">
   About the Authors
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/training/gradient_descent.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/source-separation/tutorial"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/source-separation/tutorial/issues/new?title=Issue%20on%20page%20%2Ftraining/gradient_descent.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/source-separation/tutorial/master?urlpath=tree/book/training/gradient_descent.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/source-separation/tutorial/blob/master/book/training/gradient_descent.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loss-functions">
   Loss functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#brute-force-approach">
   Brute-force approach
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Gradient descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#impact-of-learning-rate">
   Impact of learning rate
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#signs-of-healthy-training">
   Signs of healthy training
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#enough-rambling-what-do-i-use">
   Enough rambling - what do i use?!
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimizer-choice">
     Optimizer choice
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-clipping">
     Gradient clipping
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#autoclip">
       AutoClip
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="gradient-descent">
<h1>Gradient descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h1>
<p>The goal of this chapter is to provide a quick overview of gradient descent
based optimization and how it interacts with deep source separation models.
Gradient descent is how nearly all modern deep nets are trained.
Many more in-depth resources exist out there, as this chapter will only
scratch the surface. The learning outcomes of this chapter are:</p>
<ol class="simple">
<li><p>Understand at a high-level what gradient descent is doing and how it works.</p></li>
<li><p>Understand and be able to choose between different optimization algorithms.</p></li>
<li><p>Be able to investigate various aspects of the learning process for debugging,
diagnosing issues in your training scripts, or intellectual curiosity.</p></li>
</ol>
<p>First, let’s set up a simple example through which we can investigate
gradient descent. Let’s learn a simple linear regression. There are more
straightforward ways to learn linear regression, but for the sake of
pedagogy, we’ll start with this simple problem. We’ll use PyTorch as our
ML library for this.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%%capture
!pip install scaper
!pip install nussl
!pip install git+https://github.com/source-separation/tutorial
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">nussl</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">gif</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">tqdm</span>

<span class="n">nussl</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">to_numpy</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">to_tensor</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">show_gif</span><span class="p">(</span><span class="n">frames</span><span class="p">,</span> <span class="n">duration</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">600</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">(</span><span class="n">suffix</span><span class="o">=</span><span class="s1">&#39;.gif&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">gif</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">frames</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">duration</span><span class="o">=</span><span class="n">duration</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">,</span><span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">im</span> <span class="o">=</span> <span class="n">Image</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">(),</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">im</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Let’s make a single layer neural network with 1 hidden unit. This
is just a line, and corresponds to:</p>
<div class="math notranslate nohighlight">
\[y = mx + b\]</div>
<p>where <span class="math notranslate nohighlight">\(y\)</span> is the output of the network, and <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are
learnable parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Line</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>
    
<span class="n">line</span> <span class="o">=</span> <span class="n">Line</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">line</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Prediction of network at iteration. 0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">to_numpy</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">to_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_descent_5_0.png" src="../_images/gradient_descent_5_0.png" />
</div>
</div>
<p>The network is randomly initialized, and we passed some random data through it.
Since it’s a single linear layer with one unit, we can see that it is a line.
The magic is hidden away inside the <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> call. PyTorch initializes
a single network layer with one hidden unit (<span class="math notranslate nohighlight">\(m\)</span>) and a bias (<span class="math notranslate nohighlight">\(b\)</span>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">line</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>layer.weight Parameter containing:
tensor([[-0.0075]], requires_grad=True)
layer.bias Parameter containing:
tensor([0.5364], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">layer.weight</span></code> corresponds to <span class="math notranslate nohighlight">\(m\)</span> and <code class="docutils literal notranslate"><span class="pre">layer.bias</span></code> corresponds to <span class="math notranslate nohighlight">\(b\)</span>. Note
the way we are iterating over the parameters in the network - that’ll be
important later on!</p>
<p>Now that we’ve got our simple model, let’s make some training data. The training
data here will be a line of some slope with some bias, plus a bit of random
noise with a mean of <span class="math notranslate nohighlight">\(0\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>. In math, it’s like this:</p>
<div class="math notranslate nohighlight">
\[y = mx + b + \mathcal{N}(0, \sigma)\]</div>
<p>We will try to recover <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(b\)</span> as close as possible via gradient
descent. Okay, let’s make the data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">noise</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Training data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_descent_9_0.png" src="../_images/gradient_descent_9_0.png" />
</div>
</div>
<p>Let’s look at what our network does on this data, overlaid with the actual training data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">line</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Training data + network predictions&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">to_numpy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Network predictions&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_descent_11_0.png" src="../_images/gradient_descent_11_0.png" />
</div>
</div>
</div>
<div class="section" id="loss-functions">
<h2>Loss functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h2>
<p>Let’s now take a look at how gradient descent can be used to learn <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(b\)</span>
directly from the training data. The first thing we need is a way to tell how
well the network is doing right now. That is to say, how accurate are its predictions?
To do this, we need a loss function. A very simple one would just be to take the
absolute difference between the predictions and the ground truth:</p>
<div class="math notranslate nohighlight">
\[L(x, y; \theta) = |\theta(x) - y|^1_1\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta\)</span> is our neural network function, which does the following operation:</p>
<div class="math notranslate nohighlight">
\[ \theta(x) = \hat{m}x + \hat{b} \]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{m}\)</span> and <span class="math notranslate nohighlight">\(\hat{b}\)</span> are the current parameters of the network.</p>
<p>So, how’s our network doing?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">to_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Loss for each data point&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">to_numpy</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_descent_14_0.png" src="../_images/gradient_descent_14_0.png" />
</div>
</div>
<p>Above you can see the loss for every point in our training data. But in order to
do the next step, we will need to represent the performance of the network as a
single number. To do this, we’ll just take the mean:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(8.9430, grad_fn=&lt;MeanBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Why the mean and not the sum or some other aggregator? Well, the mean is nice because
it stays in the same range no matter how many data points you compute the loss over,
unlike the sum. Second, we want to increase the performance across the board, so we wouldn’t want
to use max or some operation that only looks at one data point.</p>
</div>
<div class="section" id="brute-force-approach">
<h2>Brute-force approach<a class="headerlink" href="#brute-force-approach" title="Permalink to this headline">¶</a></h2>
<p>Now that we’ve got a measure of how well our network is doing, how do we make the
network better? The goal is to reduce the loss. Let’s do this in a really naive way
first: let’s guess! We’ll do a search over all the possible network parameters for our
Line module within some range, and compute the loss for each one:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">possible_m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">possible_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span>
    <span class="n">possible_m</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> 
    <span class="n">possible_b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">m_hat</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">possible_m</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">b_hat</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">possible_b</span><span class="p">):</span>
        <span class="n">line</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">m_hat</span>
        <span class="n">line</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">b_hat</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">line</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">to_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">_loss</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;2D Visualization of Loss Landscape&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">possible_m</span><span class="p">,</span> <span class="n">possible_b</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">shading</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Value of m&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Value of b&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_descent_18_0.png" src="../_images/gradient_descent_18_0.png" />
</div>
</div>
<p>What we did: iterate over all values of <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(b\)</span> above and compute the loss.
Then, we plotted the loss in a 2D visualization. We can see the dark blue part of
the image, which indicates where the loss is minimized. Let’s see what the actual
value is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unravel_index</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span> <span class="n">loss</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">m_hat</span> <span class="o">=</span> <span class="n">possible_m</span><span class="p">[</span><span class="n">idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
<span class="n">b_hat</span> <span class="o">=</span> <span class="n">possible_b</span><span class="p">[</span><span class="n">idx</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loss minimum of </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">:</span><span class="s2">-2f</span><span class="si">}</span><span class="s2"> at </span><span class="se">\n\t</span><span class="s2"> m_hat=</span><span class="si">{</span><span class="n">m_hat</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">, b_hat=</span><span class="si">{</span><span class="n">b_hat</span><span class="si">:</span><span class="s2">-.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Actual m, b: </span><span class="se">\n\t</span><span class="s2"> m=</span><span class="si">{</span><span class="n">m</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, b=</span><span class="si">{</span><span class="n">b</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loss minimum of 0.086866 at 
	 m_hat=1.76, b_hat=0.38
Actual m, b: 
	 m=1.76, b=0.40
</pre></div>
</div>
</div>
</div>
<p>Here’s what the network predictions look like, with the learned line:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">line</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">m_hat</span>
<span class="n">line</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">b_hat</span>

<span class="n">y_hat</span> <span class="o">=</span> <span class="n">line</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Training data + network predictions&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">to_numpy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Network predictions&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_descent_22_0.png" src="../_images/gradient_descent_22_0.png" />
</div>
</div>
<p>Iterating over all possible <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(b\)</span> values in a range worked here, but it’s not a great
thing in general. We only have <span class="math notranslate nohighlight">\(2\)</span> parameters here, and we did <span class="math notranslate nohighlight">\(100\)</span> choices for each, so that worked
out to <span class="math notranslate nohighlight">\(100 * 100\)</span> = 10k “iterations” to learn a line! How do we cut this down? By using
gradient descent, of course.</p>
</div>
<div class="section" id="id1">
<h2>Gradient descent<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>In gradient descent, we look at the local loss landscape - where we are now and the points that we
could go to around us. To make things simpler, let’s use the data we generated above to look
at how the loss changes as the value of <span class="math notranslate nohighlight">\(m\)</span> changes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">possible_m</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Value of m&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Loss as a function of m&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_descent_24_0.png" src="../_images/gradient_descent_24_0.png" />
</div>
</div>
<p>Now, let’s look at it as <span class="math notranslate nohighlight">\(b\)</span> changes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">possible_b</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Value of b&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Loss as a function of b&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_descent_26_0.png" src="../_images/gradient_descent_26_0.png" />
</div>
</div>
<p>The slope of these curves is the gradient. For example, in the first <span class="math notranslate nohighlight">\(m\)</span> plot, we
see the loss goes down as <span class="math notranslate nohighlight">\(m\)</span> increases from <span class="math notranslate nohighlight">\(-1\)</span> to <span class="math notranslate nohighlight">\(-.75\)</span> roughly linearly. The
gradient between these points is simply the change in the loss with respect to <span class="math notranslate nohighlight">\(m\)</span>
as you change it from <span class="math notranslate nohighlight">\(-1\)</span> to <span class="math notranslate nohighlight">\(-.75\)</span>: about <span class="math notranslate nohighlight">\(-1\)</span>. By using the gradient to continue
in the direction that makes the loss go down, we are doing gradient descent. Note that
at the minima - where the loss is lowest, the gradient is <span class="math notranslate nohighlight">\(0\)</span>.</p>
<p>PyTorch has an easy way of computing gradients: the <code class="docutils literal notranslate"><span class="pre">backward()</span></code> function. To compute
the gradients, just compute the loss and then call <code class="docutils literal notranslate"><span class="pre">backward()</span></code> on it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">line</span> <span class="o">=</span> <span class="n">Line</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">line</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">m_hat</span>
    <span class="n">line</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">b_hat</span>

<span class="n">y_hat</span> <span class="o">=</span> <span class="n">line</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">to_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="n">line</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">line</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[-0.8747]]), tensor([-0.1000]))
</pre></div>
</div>
</div>
</div>
<p>So the weight has a gradient flowing through it, as does the bias. Let’s do the same thing we
did for the loss before, but this time let’s look at the gradients as <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(b\)</span> change:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">possible_m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">possible_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">grad_m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span>
    <span class="n">possible_m</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> 
    <span class="n">possible_b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="p">))</span>
<span class="n">grad_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span>
    <span class="n">possible_m</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> 
    <span class="n">possible_b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">m_hat</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">possible_m</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">b_hat</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">possible_b</span><span class="p">):</span>
        <span class="n">line</span> <span class="o">=</span> <span class="n">Line</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">line</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">m_hat</span>
            <span class="n">line</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">b_hat</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">line</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">to_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        
        <span class="n">grad_m</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">grad_b</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">possible_m</span><span class="p">,</span> <span class="n">grad_m</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Value of m&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Gradient&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Gradient as a function of m&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">possible_b</span><span class="p">,</span> <span class="n">grad_b</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Value of b&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Gradient&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Gradient as a function of b&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_descent_30_0.png" src="../_images/gradient_descent_30_0.png" />
<img alt="../_images/gradient_descent_30_1.png" src="../_images/gradient_descent_30_1.png" />
</div>
</div>
<p>Above we can see that at each value of <span class="math notranslate nohighlight">\(m\)</span> or <span class="math notranslate nohighlight">\(b\)</span> the gradient tells
us which way will <em>increase</em> the loss. Below the optimal value of <span class="math notranslate nohighlight">\(\hat{m}\)</span>,
it’s telling us that decreasing <span class="math notranslate nohighlight">\(\hat{m}\)</span> will <em>increase</em> the loss. So therefore, we
must go in the <em>opposite</em> direction of the gradient. Let’s put it all together:</p>
<ol class="simple">
<li><p>Compute the gradient for the current network parameters <span class="math notranslate nohighlight">\(\hat{m}\)</span> and <span class="math notranslate nohighlight">\(\hat{b}\)</span>.</p></li>
<li><p>Go in the opposite direction of the gradient by some fixed amount.</p></li>
<li><p>Go back to 1.</p></li>
</ol>
<p>In a simple for loop, it looks like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N_ITER</span><span class="o">=</span><span class="mi">100</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="c1"># initialize line</span>
<span class="n">line</span> <span class="o">=</span> <span class="n">Line</span><span class="p">()</span>

<span class="n">frames</span> <span class="o">=</span> <span class="p">[]</span>
<span class="nd">@gif</span><span class="o">.</span><span class="n">frame</span>
<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">line</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Training data + network predictions</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="sa">f</span><span class="s2">&quot;Learning rate is </span><span class="si">{</span><span class="n">LEARNING_RATE</span><span class="si">}</span><span class="s2">, Iteration </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training data&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">to_numpy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Network predictions&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_ITER</span><span class="p">):</span>
    <span class="n">line</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">line</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">to_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">line</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">LEARNING_RATE</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
    
    <span class="n">frame</span> <span class="o">=</span> <span class="n">plot</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">frames</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>

<span class="n">show_gif</span><span class="p">(</span><span class="n">frames</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_descent_32_0.png" src="../_images/gradient_descent_32_0.png" />
</div>
</div>
</div>
<div class="section" id="impact-of-learning-rate">
<h2>Impact of learning rate<a class="headerlink" href="#impact-of-learning-rate" title="Permalink to this headline">¶</a></h2>
<p>The key hyperparameter to consider here is the learning rate. The learning rate
controls how big of a step you take in the direction away from the gradient.
Let’s see how this parameter can affect the performance of gradient descent, by
trying a few different values, and visualizing the learning process for each one:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N_ITER</span><span class="o">=</span><span class="mi">100</span>
<span class="n">LEARNING_RATES</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">]</span>

<span class="n">line</span> <span class="o">=</span> <span class="n">Line</span><span class="p">()</span>
<span class="n">lines</span> <span class="o">=</span> <span class="p">[</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">LEARNING_RATES</span><span class="p">]</span>

<span class="n">losses</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">LEARNING_RATES</span><span class="p">))]</span>
<span class="n">grad_norms</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">LEARNING_RATES</span><span class="p">))]</span>

<span class="n">frames</span> <span class="o">=</span> <span class="p">[]</span>
<span class="nd">@gif</span><span class="o">.</span><span class="n">frame</span>
<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">lines</span><span class="p">):</span>
    <span class="n">ncols</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">nrows</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">LEARNING_RATES</span><span class="p">)</span> <span class="o">//</span> <span class="n">ncols</span>
    <span class="n">width</span> <span class="o">=</span> <span class="n">ncols</span> <span class="o">*</span> <span class="mi">5</span>
    <span class="n">height</span> <span class="o">=</span> <span class="n">nrows</span> <span class="o">*</span> <span class="mi">4</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">))</span>
    <span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lines</span><span class="p">):</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">line</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Training data + network predictions</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Learning rate is </span><span class="si">{</span><span class="n">LEARNING_RATES</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="si">}</span><span class="s2">, Iteration </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training data&#39;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">to_numpy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Network predictions&#39;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
        
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_ITER</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lines</span><span class="p">):</span>
        <span class="n">line</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">line</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">to_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">losses</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_loss</span><span class="p">)</span>
        <span class="n">_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        
        <span class="n">grad_norm</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">line</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">LEARNING_RATES</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">grad_norm</span> <span class="o">+=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">grad_norm</span> <span class="o">=</span> <span class="n">grad_norm</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">grad_norms</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad_norm</span><span class="p">)</span>

    <span class="n">frame</span> <span class="o">=</span> <span class="n">plot</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">lines</span><span class="p">)</span>
    <span class="n">frames</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>

<span class="n">show_gif</span><span class="p">(</span><span class="n">frames</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">1200</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_descent_34_0.png" src="../_images/gradient_descent_34_0.png" />
</div>
</div>
<p>If the learning rate is set too high, then the correct solution is never reached as the steps being taken are much
too large. This results in the optimization oscillating back and forth between different
parameters. At the more optimal learning rate of 0.01, the optimizations succeeds in finding the
best parameters. At too low of a learning rate (0.001 and below), the optimization will eventually
reach the optimal point but will be very inefficient in getting there.</p>
<p>With the optimal learning rate, our model learns the true data distribution within 25 iterations. Much
more efficient than the 10k iterations for brute forcing!</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You’ll always want your learning rate to be set as high as possible, but not so high that
optimization becomes unstable. Lower learning rates are generally “safer” in terms
of reaching minima, but are more inefficient. Soon, we’ll look at ways that you can
monitor the health of your training procedure and how that help guide your choices
for optimization hyperparameters.</p>
</div>
</div>
<div class="section" id="signs-of-healthy-training">
<h2>Signs of healthy training<a class="headerlink" href="#signs-of-healthy-training" title="Permalink to this headline">¶</a></h2>
<p>The network that we’ve looked at so far is an exceedingly simple one. Deep audio
models are of course not single one-weight layer networks. Much of
the analysis that we’ve done so far is not possible in high dimensions. There are
essentially two core tools that one can use to diagnose and monitor network
training:</p>
<ul class="simple">
<li><p>The training and validation loss</p></li>
<li><p>The gradient norm</p></li>
</ul>
<p>By monitoring these two metrics, one can get a good idea of whether the learning rate
is set too high or too low, whether different optimization algorithms should be used, etc.</p>
<p>Let’s examine the behavior of these for each of the 6 learning rates above. In that code,
we saved the loss history as well as the norm of the gradient at each iteration.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">LEARNING_RATES</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grad_norms</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Learning rate: </span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;grad norm&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Gradient norm for each learning rate&#39;</span><span class="p">)</span>
    
<span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">LEARNING_RATES</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">losses</span><span class="p">[</span><span class="n">j</span><span class="p">]),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Learning rate: </span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;log(loss)&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Loss for each learning rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gradient_descent_37_0.png" src="../_images/gradient_descent_37_0.png" />
</div>
</div>
<p>How do we interpret these plots? The plot on the right tells us that for some learning rates,
we get an oscillating behavior in the loss, suggesting we are jumping rapidly from very different
points in the space. The plot on the left suggests that we start from a place that has very high
gradient norm. Because of this, combined with the high learning rate, we get this oscillating behavior.
The optimal learning rate starts with high gradient norm, but as it moves closer to the minima, the
gradient norm decreases, indicating healthy training.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>When training your own networks, even big ones, it can be really helpful to look at these plots
and adjust your hyperparameters accordingly.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There is extensive research into better optimization procedures. Here we are looking at SGD (Stochastic
Gradient Descent), but in practice we will use a momentum-based optimizer.</p>
</div>
</div>
<div class="section" id="enough-rambling-what-do-i-use">
<h2>Enough rambling - what do i use?!<a class="headerlink" href="#enough-rambling-what-do-i-use" title="Permalink to this headline">¶</a></h2>
<p>In any deep learning project, it’s easy to get bogged down in “hyperparameter hell”, where
the loss landscape is spikey, scary, and full of nightmares. Sometimes, it seems that
no matter what you do, the loss simply won’t go down, or it won’t reach the loss
reported in paper X, etc. We’ve all been there. In this section, the goal is to
introduce the most common settings that are in a lot of different source separation
papers.</p>
<div class="section" id="optimizer-choice">
<h3>Optimizer choice<a class="headerlink" href="#optimizer-choice" title="Permalink to this headline">¶</a></h3>
<p>The choice of optimizer is most often the ADAM optimizer. ADAM is an optimizer which
traverses the loss landscape using <em>momentum</em>. If you’re new to momentum, here’s a
fantastic resource to get acquainted with it: https://distill.pub/2017/momentum/.</p>
<p>In momentum-based optimization, the idea is to adaptively change the learning rate
based on the <em>history</em> of the gradients. If gradients are small, but they’re always
pointing in the same direction as you traverse the loss landscape, that indicates that
you can take bigger steps! Bigger steps means more efficient learning. ADAM codifies
this logic via some math that we don’t have to get into here. It suffices to understand
that the idea of ADAM is that more consistent gradients leads to faster learning. To
quote the Distill post above - if gradient descent is a person walking down a hill, then
momentum is a ball rolling down a hill. The ball picks up speed as it rolls.</p>
<p>Momentum-based optimization is a very common choice in the source separation literature.
Specifically, ADAM is used with the following hyperparameters being a good initial setup
(that you likely won’t have to change):</p>
<ul class="simple">
<li><p>ADAM Optimizer</p></li>
<li><p>Learning rate: 1e-3</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_1\)</span> = .9, <span class="math notranslate nohighlight">\(\beta_2\)</span> = .999</p></li>
</ul>
<p>Lucky for you, these are the <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Adam">PyTorch defaults</a>
(funny how that works)!</p>
</div>
<div class="section" id="gradient-clipping">
<h3>Gradient clipping<a class="headerlink" href="#gradient-clipping" title="Permalink to this headline">¶</a></h3>
<p>Another popular trick to improve optimization is to use <em>gradient clipping</em>. In many loss
landscapes, the gradient is not perfectly smooth as we have seen so far. Often, the loss
landscape is exceedingly noisy, with many small bumps and imperfections. These imperfections
can lead to huge spikes in the gradient, which can destabilize the learning process.</p>
<p>The damage that huge spikes in the loss landscape can do to optimization can
be mitigated via gradient clipping. Simply put, in gradient clipping if the
gradient norm exceeds some set threshold, then it is renormalized such that the
norm of the gradient is equal to that threshold. If the norm is below the set
threshold, then the gradient is untouched.</p>
<p>Why does gradient clipping work so well, theoretically? Well, it’s a bit of an
open question right now with a long history <a class="bibtex reference internal" href="../zzz_refs.html#bengio1994learning" id="id2">[BSF94]</a><a class="bibtex reference internal" href="../zzz_refs.html#mikolov2012statistical" id="id3">[Mik12]</a>,
and a lot of interesting recent work <a class="bibtex reference internal" href="../zzz_refs.html#zhang2019gradient" id="id4">[ZHSJ19]</a>!</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><em>Exploding gradients</em>, along with <em>vanishing gradients</em> are important to look out for. Note that most
recurrent network architectures are susceptible to both types of gradient issues, and that
the best way to stabilize training and overcome exploding gradients is via gradient
clipping. Gradient clipping is an important part of the recipe for many state-of-the-art
separation networks.</p>
</div>
<div class="section" id="autoclip">
<h4>AutoClip<a class="headerlink" href="#autoclip" title="Permalink to this headline">¶</a></h4>
<p>Picking the optimal gradient clipping threshold can be tough, and choosing it poorly can lead to bad
results. Recent work <a class="bibtex reference internal" href="../zzz_refs.html#seetharaman2020autoclip" id="id5">[SWPR20]</a> proposes an automated mechanism to choose
the gradient clipping threshold by using the history of the gradient norms in conjunction with a
simple percentile based approach. For more, see <a class="reference external" href="https://github.com/pseeth/autoclip">here</a> (full
disclosure: the author of this method is currently writing what you’re reading now).</p>
<p>In summary, use the Adam optimizer combined with some flavor of gradient clipping (either with a
hand-tuned threshold, or with AutoClip) to train your network. In the next section, we’ll start
to explore actual models, their building blocks, and how things are put together.</p>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "source-separation/tutorial",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./training"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="intro.html" title="previous page">Introduction</a>
    <a class='right-next' id="next-link" href="building_blocks.html" title="next page">Coding up model architectures</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Ethan Manilow, Prem Seetharaman, Justin Salamon<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-180111316-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>