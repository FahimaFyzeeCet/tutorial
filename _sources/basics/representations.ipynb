{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representing Audio\n",
    "==================\n",
    "\n",
    "<p align=\"center\">\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/FTQbiNvZqaY\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</p>\n",
    "\n",
    "```{dropdown} Video not working?\n",
    "If you are having access issues, here is a OneDrive mirror to the full video.\n",
    "<a href=\"\"></a>\n",
    "\n",
    "Alternatively the text on this page covers the same material as the video.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "other": {
     "more": true
    },
    "tags": [
     "hide-output",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Inputs for this notebook\n",
    "import nussl\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to examine are the input and output representations of a source \n",
    "separation system and how the inputs and outputs are represented. In its most\n",
    "unprocessed form, we assume that audio is stored as a waveform. Some source\n",
    "separation approaches operate on the waveform directly, although many require\n",
    "some preprocessing before separating sources. In this section, we will discuss\n",
    "the different types of input and output representations that are commonly used\n",
    "in source separation approaches.\n",
    "\n",
    "Many of the details of this section can be found other places. We\n",
    "encourage the curious to follow the resources on this page to learn more.\n",
    "\n",
    "Some great resources for further reading:\n",
    "\n",
    "- [Music Information Retrieval](https://musicinformationretrieval.com/) website\n",
    "- [Prof. Bryan Pardo's class on Machine Perception of Music](https://interactiveaudiolab.github.io/teaching/eecs352.html)\n",
    "- [Prof. Alexander Lerch's Audio Content Analysis](https://www.audiocontentanalysis.org/) website\n",
    "\n",
    "\n",
    "## Waveforms\n",
    "\n",
    "\n",
    "```{figure} ../images/basics/richter.gif\n",
    "---\n",
    "alt: Waveform shown at many different time scales from a few seconds to a few samples.\n",
    "name: waveform-gif\n",
    "---\n",
    "A waveform shown at many different time scales. Each value is sampled at a uniform\n",
    "rate and quantized. Image used courtesy of Jan Van Balen (<a href=\"https://jvbalen.github.io/notes/waveform.html\">source</a>).\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "A _waveform_ is shorthand for a digitized audio signal, which is most similar to\n",
    "what the sound is like physically. For an acoustic sound, the air pressure of\n",
    "over time changes, and is\n",
    "recorded by a microphone, which converts the changes in air pressure to an electrical \n",
    "signal. The voltage of this signal is sampled at a regular time interval,\n",
    "quantized, and converted to a digital array in a computer. This digital array is\n",
    "what we call the waveform. Of course, this description glosses over a lot of\n",
    "details in the realm of physics, acoustics, and signal processing. What's\n",
    "important to know is that a continuous-time signal is discretized in both time\n",
    "and amplitude. We say a signal is {term}`monophonic`, or {term}`mono`, if there\n",
    "is only one audio channel, i.e., this array has shape $x \\in \\mathbb{R}^{t \\times 1}$.\n",
    "We say a signal is {term}`stereophonic`, or {term}`stereo`, if the array has two\n",
    "channels, i.e., this array has shape $x \\in \\mathbb{R}^{t \\times 2}$.\n",
    "\n",
    "```{note}\n",
    "Audio signals with more than 2 channels have many applications (e.g.\n",
    "5.1 surround sound), including applications for in separating sources.\n",
    "Approaches that input many audio channels for separation are typically refered\n",
    "to under the title of _beamforming_ approaches. Beamforming is a related, but\n",
    "separate area of active research. As such, the work we are going to cover in\n",
    "this space is sometimes called _Single Channel Source Separation_.\n",
    "```\n",
    "\n",
    "An important aspect of the waveform is the {term}`sample rate`, which describes how\n",
    "many measurements, or {term}`sample`s, happen per second and is measured in Hertz, or\n",
    "Hz[^fn2]. For a signal with sample rate $sr$, the maximum frequency that can be\n",
    "reliably represented is $f_N=\\frac{sr}{2}$, which is called the\n",
    "[Nyquist frequency](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem).\n",
    "For example, if a signal has a sample rate of 44.1 kHz, the highest\n",
    "possible frequency is 22.05 kHz.\n",
    "\n",
    "Many deep learning-based source separation approaches will reduce the sample\n",
    "rate of their input signals (called {term}`downsampling`) to reduce the\n",
    "computational load during training time. Downsampling removes high frequency\n",
    "information from a signal, which is seen as a necessary evil to increase the \n",
    "overall performance of an approach. Most of the frequencies above 10 kHz are\n",
    "only harmonics or partials; while these are certainly important for perception,\n",
    "researchers have found that ensuring the lower frequencies sound better \n",
    "[WHAT DO I WANT TO SAY HERE????]\n",
    "\n",
    "```{note}\n",
    "All of the source separation approaches we will discuss assume that the sample\n",
    "rate between the training, validation, and testing data is the same. The\n",
    "assumptions that ensure the approaches work are violated if the sample rate is\n",
    "variable. For example, if a system expects a signal at 16 kHz, then\n",
    "all input audio should be resampled to 16 kHz before using it.\n",
    "```\n",
    "\n",
    "\n",
    "## Desirable Properties of Representations\n",
    "\n",
    "It can be argued that a source separation approach is only as good as its ability\n",
    "to represent audio in a separable manner. With that in mind, it's important\n",
    "to understand how audio itself is represented for the purposes of source separation.\n",
    "For the source separation approaches we will explore here, we will see many\n",
    "variations on the same theme, namely:\n",
    "\n",
    "1. Convert the audio to a representation easy to separate\n",
    "2. Separate the audio by manipulating this representation\n",
    "3. Convert the audio back from the manipulated representation to get isolated sources.\n",
    "\n",
    "Almost every source separation approach we discuss here--classic and deep--can\n",
    "be broken down into these three steps. We want to note that each of these three \n",
    "steps might in fact involve multiple separate substeps.\n",
    "(The astute reader may notice some similarities between this framing and\n",
    "[Support Vector Machines](https://en.wikipedia.org/wiki/Support_vector_machine).)\n",
    "\n",
    "Therefore an important aspect of an audio representation is {term}`invertability`, or\n",
    "whether a signal that is converted from a waveform to a new representation can\n",
    "reliably be converted back to a waveform with little-to-no error. Artifacts that\n",
    "arise from converting back and forth will be audible in our separation output, so\n",
    "we want to minimize these types of errors (although eliminating them does not\n",
    "guarantee a perfect separation).\n",
    "\n",
    "An other important aspect is whether this representation can keep data from \n",
    "one source apart from another source.\n",
    "\n",
    "[[FATEMEH'S EXAMPLES]]\n",
    "\n",
    "Some recent source separations approaches use deep learning to learn a representation\n",
    "directly from the waveform, while others use preprocessing tools that are common \n",
    "in the audio signal processing and music information retrieval literature as a first\n",
    "step.\n",
    "\n",
    "\n",
    "## Input Representations\n",
    "\n",
    "\n",
    "\n",
    "### Time-Frequency (TF) Representations\n",
    "\n",
    "[[IMAGE]]\n",
    "\n",
    "A Time-Frequency {cite}`smith2011spectral` representation is a 2 dimensional\n",
    "matrix that represents the frequency contents of an audio signal over time.\n",
    "There are many types of time-frequency representations out in the world, but we\n",
    "will only discuss those that are most frequently used for source separation here.\n",
    "\n",
    "We call a specific entry in this matrix a {term}`TF bin`. We can visualize a \n",
    "{term}`TF-Representation` using a heatmap, which has time along the x-axis and\n",
    "frequency along the y-axis. Each TF bin in the heatmap represents the amplitude\n",
    "of the signal at that particular time and frequency. Some heatmaps have a colorbar\n",
    "alongside them that shows which colors indicate high amplitude values and which\n",
    "colors indicate low amplitude values. If there is no color bar, it is usually\n",
    "safe to assume that brighter colors indicate higher amplitudes than darker colors.\n",
    "\n",
    "Time-frequency representations are the most common types of representations used\n",
    "in source separation approaches. Below we will outline some of the most popular\n",
    "and fundamental time-frequency representations.\n",
    "\n",
    "\n",
    "#### Short-time Fourier Transform (STFT)\n",
    "\n",
    "```{figure} ../images/basics/stft_process.png\n",
    "---\n",
    "alt: Diagram depicting how a short-time Fourier transform is computed.\n",
    "name: stft_process\n",
    "---\n",
    "The process of computing a short-time Fourier transform of a waveform. Imaged used\n",
    "courtesy of Bryan Pardo.\n",
    "```\n",
    "\n",
    "Many of the time-frequency representations that we will see in this tutorial\n",
    "start out as a Short-time Fourier Transform or STFT. An STFT is calculated \n",
    "from a waveform representation by computing a \n",
    "[discrete Fourier transform](https://en.wikipedia.org/wiki/Discrete_Fourier_transform)\n",
    "(DFT) of a small, moving window[^fn1] across the duration of the window. The location\n",
    "of each entry in an STFT determines its time (x-axis) and frequency (y-axis). The\n",
    "absolute value of a TF bin $|X(t, f)|$ at time $t$ and frequency $f$ determines the \n",
    "amount of energy heard from frequency $f$ at time $t$.\n",
    "\n",
    "Importantly, each bin in our STFT is _complex_, meaning each entry contains both\n",
    "a magnitude component and a phase component. Both components are needed to convert\n",
    "an STFT matrix back to a waveform so that we may hear it.\n",
    "\n",
    "Here are some important parameters to consider when computing an STFT:\n",
    "\n",
    "##### Window Types\n",
    "\n",
    "[IMAGE]\n",
    "\n",
    "The window type determines the shape of the short-time window that will segment\n",
    "the audio into short segments before applying the DFT. The shape of this window\n",
    "can will affect which frequencies get emphasized or attenuated in the DFT.\n",
    "\n",
    "We recommend you use []\n",
    "\n",
    "##### Window Length\n",
    "\n",
    "[IMAGE]\n",
    "\n",
    "The window length determines how many samples are included in each short-time\n",
    "window. Due to how the DFT is computed, this parameter also determines the\n",
    "resolution of the frequency axis of the STFT. The longer the window, the higher\n",
    "the frequency resolution and vice versa.\n",
    "\n",
    "##### Hop Length\n",
    "\n",
    "The hop length determines the distance, in samples, between any two adjacent\n",
    "short-time windows.\n",
    "\n",
    "##### Other Considerations\n",
    "\n",
    "- Overlap add\n",
    "\n",
    "\n",
    "#### Magnitude Spectrograms\n",
    "\n",
    "[IMAGE]\n",
    "\n",
    "As we will touch on later in this tutorial, it is hard to model the\n",
    "phase of a signal. Therefore most source separation approaches only operate on\n",
    "the some variant of the spectrogram that does not explicitly represent phase in\n",
    "each {term}`TF bin`. The first of these variants we'll look at is the Magnitude\n",
    "Spectrogram.\n",
    "\n",
    "For a complex-valued STFT, $X \\in \\mathbb{C}^{T \\times F}$, the Magnitude\n",
    "Spectrogram is calculated by taking the absolute value of each element in the\n",
    "STFT, $|X| \\in \\mathbb{R}^{T \\times F}$. \n",
    "\n",
    "\n",
    "Phase is necessary to reconstruct the signal, and we\n",
    "will discuss how it's dealt with in a later section.\n",
    "\n",
    "\n",
    "```{note}\n",
    "A note on terminology: while researchers might loosely interchange \"STFT\" and\n",
    "\"spectrogram\", the term \"spectrogram\" is mostly used to describe a TF Representation\n",
    "that does not have any explicit phase representation. As such \"spectrogram\" might\n",
    "refer to a Magnitude Spectrogram, Power Spectrogram, Log Spectrogram, Mel Spectrogram,\n",
    "Log Mel Spectrogram, or similar. Use context clues to determine which representation\n",
    "is being discussed when possible.\n",
    "```\n",
    "\n",
    "#### Power Spectrograms\n",
    "\n",
    "[IMAGE]\n",
    "\n",
    "Similar to the Magnitude Spectrogram, the Power Spectrogram only contains information\n",
    "about the amplitude of a signal.\n",
    "\n",
    "For a complex-valued STFT, $X \\in \\mathbb{C}^{T \\times F}$, the Power\n",
    "Spectrogram is calculated by squaring  each element in the\n",
    "STFT, $|X|^2 \\in \\mathbb{R}^{T \\times F}$. \n",
    "\n",
    "\n",
    "\n",
    "#### Log Spectrograms\n",
    "\n",
    "[IMAGE]\n",
    "\n",
    "Human hearing is logarithmic with regards to amplitude. To compute a log spectrogram,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Mel-spaced Spectrograms\n",
    "\n",
    "[IMAGE]\n",
    "\n",
    "Human hearing is also logarithmic with regards to frequencies. The \n",
    "[Mel scale](https://en.wikipedia.org/wiki/Mel_scale) approximates this property\n",
    "and is a quick way to make the frequency axis of a spectrogram \n",
    "quasi-logarithmic[^fn3]. This is also commonly used to reduce the computational\n",
    "load on deep learning-based approaches, because the number of Mel-spaced frequency\n",
    "bins is often lower then the number of linearly-spaced frequency bins.\n",
    "\n",
    "#### Log Mel-spaced Spectrograms\n",
    "\n",
    "[IMAGE]\n",
    "\n",
    "Combine Mel representations and \n",
    "\n",
    "\n",
    "### Other Representations\n",
    "\n",
    "A few other representations have been explored in the literature. We provide references\n",
    "for a few below:\n",
    "\n",
    "- Constant-Q Transform (CQT)\n",
    "  - General Calculation: {cite}`brown1991calculation,brown1992efficient`\n",
    "  - Use in source separation: {cite}`rafii2011degenerate,fuentes2012blind,jaiswal2011clustering,ganseman2012improving,shi2019cqt`\n",
    "- Common Fate Transform (CFT) {cite}`stoter2016common`\n",
    "- Multi-resolution Common Fate Transform (MCFT) {cite}`pishdadian2018multi`\n",
    "- 2-Dimensional Fourier Transform (2DFT) {cite}`seetharaman2017music`\n",
    "- Gabor Transform \n",
    "- Per Channel Energy Normalization (PCEN) {cite}`lostanlen2018per`\n",
    "- Wavelets?\n",
    "- KAM\n",
    "\n",
    "\n",
    "## Output Representations\n",
    "\n",
    "Ultimately, all source separation algorithms must be able to convert the audio\n",
    "that they processed back to a waveform. While some algorithms output waveforms\n",
    "directly, many algorithms output masks, which will be covered in more detail in\n",
    "the next sections. The masks get applied to the original mixture spectrogram,\n",
    "and that result is converted back to a waveform.\n",
    "\n",
    "One thing to note is that if I have a waveform estimate for Source $i$ from my\n",
    "mixture, then it is easy to calculate what the mixture sounds like _without_ \n",
    "Source $i$ present. Simply element-wise subtract the source waveform from the\n",
    "mixture waveform.\n",
    "\n",
    "\n",
    "## Which is Better? Inputting a Waveform or a Time-Frequency Representation?\n",
    "\n",
    "```{note}\n",
    "The details of this answer will be covered later in this tutorial. Feel free to\n",
    "come back to this question after you have read the first two sections.\n",
    "```\n",
    "\n",
    "It depends! Very few non-deep learning source separation approaches operate directly\n",
    "on waveforms, and there are a growing number \n",
    "\n",
    "The scientific work on evaluating source separation approaches\n",
    "is happening with regards to mixtures of speech signals, this does give us some\n",
    "indication of how \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```{bibliography} book/zreferences.bib\n",
    "```\n",
    "\n",
    "[^fn1]: This window is where the term \"Short-time\" comes from in the name\n",
    " \"Short-time Fourier Transform\".\n",
    " \n",
    "[^fn2]: Named after <a href=\"https://en.wikipedia.org/wiki/Heinrich_Hertz\">Heinrich Hertz</a>,\n",
    " who proved the existence of electromagnetic waves.\n",
    " \n",
    "[^fn3]: `librosa`, arguably the most commonly used python library for\n",
    " MIR work, has two ways to convert from Hz to Mel, which are slightly\n",
    " different: <a href=\"https://librosa.org/doc/latest/_modules/librosa/core/convert.html#hz_to_mel\">see here.</a>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "source_map": [
   11,
   26,
   36
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}