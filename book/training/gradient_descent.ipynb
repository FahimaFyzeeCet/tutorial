{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "The goal of this chapter is to provide a quick overview of gradient descent \n",
    "based optimization and how it interacts with deep source separation models. \n",
    "Gradient descent is how nearly all modern deep nets are trained. \n",
    "Many more in-depth resources exist out there, as this chapter will only\n",
    "scratch the surface. The learning outcomes of this chapter are:\n",
    "\n",
    "1. Understand at a high-level what gradient descent is doing and how it works.\n",
    "2. Understand and be able to choose between different optimization algorithms.\n",
    "3. Be able to investigate various aspects of the learning process for debugging,\n",
    "   diagnosing issues in your training scripts, or intellectual curiosity.\n",
    "   \n",
    "First, let's set up a simple example through which we can investigate \n",
    "gradient descent. Let's learn a simple linear regression. There are more\n",
    "straightforward ways to learn linear regression, but for the sake of \n",
    "pedagogy, we'll start with this simple problem. We'll use PyTorch as our\n",
    "ML library for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example: linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import nussl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gif\n",
    "from IPython.display import display, Image\n",
    "import tempfile\n",
    "import copy\n",
    "import tqdm\n",
    "\n",
    "nussl.utils.seed(0)\n",
    "to_numpy = lambda x: x.detach().numpy()\n",
    "to_tensor = lambda x: torch.from_numpy(x).reshape(-1, 1).float()\n",
    "\n",
    "def show_gif(frames, duration=5.0, width=600):\n",
    "    with tempfile.NamedTemporaryFile(suffix='.gif') as f:\n",
    "        gif.save(frames, f.name, duration=duration)\n",
    "        with open(f.name,'rb') as f:\n",
    "            im = Image(data=f.read(), format='png', width=width)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a single layer neural network with 1 hidden unit. This \n",
    "is just a line, and corresponds to:\n",
    "\n",
    "$$y = mx + b$$\n",
    "\n",
    "where $y$ is the output of the network, and $m$ and $b$ are \n",
    "learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Line(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.layer(x)\n",
    "        return y\n",
    "    \n",
    "line = Line()\n",
    "x = torch.randn(100, 1)\n",
    "y = line(x)\n",
    "\n",
    "plt.title(\"Prediction of network at iteration. 0\")\n",
    "plt.scatter(to_numpy(x), to_numpy(y))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is randomly initialized, and we passed some random data through it.\n",
    "Since it's a single linear layer with one unit, we can see that it is a line.\n",
    "The magic is hidden away inside the `nn.Linear` call. PyTorch initializes\n",
    "a single network layer with one hidden unit ($m$) and a bias ($b$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in line.named_parameters():\n",
    "    print(n, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`layer.weight` corresponds to $m$ and `layer.bias` corresponds to $b$. Note\n",
    "the way we are iterating over the parameters in the network - that'll be\n",
    "important later on!\n",
    "\n",
    "Now that we've got our simple model, let's make some training data. The training\n",
    "data here will be a line of some slope with some bias, plus a bit of random \n",
    "noise with a mean of $0$ and standard deviation $\\sigma$. In math, it's like this:\n",
    "\n",
    "$$y = mx + b + \\mathcal{N}(0, \\sigma)$$\n",
    "\n",
    "We will try to recover $m$ and $b$ as close as possible via gradient\n",
    "descent. Okay, let's make the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.random.randn()\n",
    "b = np.random.randn()\n",
    "x = np.linspace(-10, 10, 100)\n",
    "noise = np.random.normal(loc=0, scale=0.1, size=100)\n",
    "\n",
    "y = m*x + b + noise\n",
    "\n",
    "plt.title(\"Training data\")\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what our network does on this data, overlaid with the actual training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = line(to_tensor(x))\n",
    "\n",
    "plt.title(\"Training data + network predictions\")\n",
    "plt.scatter(x, y, label='Training data')\n",
    "plt.scatter(x, to_numpy(y_hat), label='Network predictions')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look at how gradient descent can be used to learn $m$ and $b$\n",
    "directly from the training data. The first thing we need is a way to tell how\n",
    "well the network is doing right now. That is to say, how accurate are its predictions?\n",
    "To do this, we need a loss function. A very simple one would just be to take the\n",
    "absolute difference between the predictions and the ground truth:\n",
    "\n",
    "$$L(x, y; \\theta) = |\\theta(x) - y|^1_1$$\n",
    "\n",
    "where $\\theta$ is our neural network function, which does the following operation:\n",
    "\n",
    "$$ \\theta(x) = \\hat{m}x + \\hat{b} $$\n",
    "\n",
    "where $\\hat{m}$ and $\\hat{b}$ are the current parameters of the network. \n",
    "\n",
    "So, how's our network doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (y_hat - to_tensor(y)).abs()\n",
    "plt.title(\"Loss for each data point\")\n",
    "plt.scatter(x, to_numpy(loss))\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you can see the loss for every point in our training data. But in order to\n",
    "do the next step, we will need to represent the performance of the network as a \n",
    "single number. To do this, we'll just take the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why the mean and not the sum or some other aggregator? Well, the mean is nice because\n",
    "it stays in the same range no matter how many data points you compute the loss over, \n",
    "unlike the sum. Second, we want to increase the performance across the board, so we wouldn't want\n",
    "to use max or some operation that only looks at one data point.\n",
    "\n",
    "Now that we've got a measure of how well our network is doing, how do we make the\n",
    "network better? The goal is to reduce the loss. Let's do this in a really naive way\n",
    "first: let's guess! We'll do a search over all the possible network parameters for our\n",
    "Line module within some range, and compute the loss for each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_m = np.linspace(-2, 2, 100)\n",
    "possible_b = np.linspace(-2, 2, 100)\n",
    "loss = np.empty((\n",
    "    possible_m.shape[0], \n",
    "    possible_b.shape[0]\n",
    "))\n",
    "\n",
    "for i, m_hat in enumerate(possible_m):\n",
    "    for j, b_hat in enumerate(possible_b):\n",
    "        line.layer.weight[0, 0] = m_hat\n",
    "        line.layer.bias[0] = b_hat\n",
    "        y_hat = line(to_tensor(x))\n",
    "        _loss = (y_hat - to_tensor(y)).abs().mean()\n",
    "        loss[i, j] = _loss\n",
    "\n",
    "plt.title('2D Visualization of Loss Landscape')\n",
    "plt.pcolormesh(possible_m, possible_b, loss, shading='auto')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Value of m')\n",
    "plt.ylabel('Value of b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did: iterate over all values of $m$ and $b$ above and compute the loss.\n",
    "Then, we plotted the loss in a 2D visualization. We can see the dark blue part of\n",
    "the image, which indicates where the loss is minimized. Let's see what the actual\n",
    "value is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.unravel_index(np.argmin(loss, axis=None), loss.shape)\n",
    "m_hat = possible_m[idx[0]]\n",
    "b_hat = possible_b[idx[1]]\n",
    "\n",
    "print(f\"Loss minimum of {loss.min():-2f} at \\n\\t m_hat={m_hat:0.2f}, b_hat={b_hat:-.2f}\")\n",
    "print(f\"Actual m, b: \\n\\t m={m:.2f}, b={b:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the network predictions look like, with the learned line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line.layer.weight[0, 0] = m_hat\n",
    "line.layer.bias[0] = b_hat\n",
    "\n",
    "y_hat = line(to_tensor(x))\n",
    "\n",
    "plt.title(\"Training data + network predictions\")\n",
    "plt.scatter(x, y, label='Training data')\n",
    "plt.scatter(x, to_numpy(y_hat), label='Network predictions')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating over all possible $m$ and $b$ values in a range worked here, but it's not a great\n",
    "thing in general. We only have $2$ parameters here, and we did $100$ choices for each, so that worked\n",
    "out to $100 * 100$ = 10k \"iterations\" to learn a line! How do we cut this down? By using\n",
    "gradient descent, of course.\n",
    "\n",
    "In gradient descent, we look at the local loss landscape - where we are now and the points that we\n",
    "could go to around us. To make things simpler, let's use the data we generated above to look\n",
    "at how the loss changes as the value of $m$ changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(possible_m, loss.mean(axis=1))\n",
    "plt.xlabel('Value of m')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at it as $b$ changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(possible_b, loss.mean(axis=0))\n",
    "plt.xlabel('Value of b')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slope of these curves is the gradient. For example, in the first $m$ plot, we \n",
    "see the loss goes down as $m$ increases from $-1$ to $-.75$ roughly linearly. The\n",
    "gradient between these points is simply the change in the loss with respect to $m$\n",
    "as you change it from $-1$ to $-.75$: about $-1$. By using the gradient to continue \n",
    "in the direction that makes the loss go down, we are doing gradient descent. Note that\n",
    "at the minima - where the loss is lowest, the gradient is $0$.\n",
    "\n",
    "PyTorch has an easy way of computing gradients: the `backward()` function. To compute\n",
    "the gradients, just compute the loss and then call `backward()` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = Line()\n",
    "with torch.no_grad():\n",
    "    line.layer.weight[0, 0] = m_hat\n",
    "    line.layer.bias[0] = b_hat\n",
    "\n",
    "y_hat = line(to_tensor(x))\n",
    "_loss = (y_hat - to_tensor(y)).abs().mean()\n",
    "_loss.backward()\n",
    "\n",
    "line.layer.weight.grad, line.layer.bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the weight has a gradient flowing through it, as does the bias. Let's do the same thing we\n",
    "did for the loss before, but this time let's look at the gradients as $m$ and $b$ change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_m = np.linspace(-1, 1, 100)\n",
    "possible_b = np.linspace(-1, 1, 100)\n",
    "grad_m = np.empty((\n",
    "    possible_m.shape[0], \n",
    "    possible_b.shape[0]\n",
    "))\n",
    "grad_b = np.empty((\n",
    "    possible_m.shape[0], \n",
    "    possible_b.shape[0]\n",
    "))\n",
    "\n",
    "for i, m_hat in enumerate(possible_m):\n",
    "    for j, b_hat in enumerate(possible_b):\n",
    "        line = Line()\n",
    "        with torch.no_grad():\n",
    "            line.layer.weight[0, 0] = m_hat\n",
    "            line.layer.bias[0] = b_hat\n",
    "        y_hat = line(to_tensor(x))\n",
    "        _loss = (y_hat - to_tensor(y)).abs().mean()\n",
    "        _loss.backward()\n",
    "        \n",
    "        grad_m[i, j] = line.layer.weight.grad.item()\n",
    "        grad_b[i, j] = line.layer.bias.grad.item()\n",
    "\n",
    "plt.plot(possible_m, grad_m.mean(axis=1))\n",
    "plt.xlabel('Value of m')\n",
    "plt.ylabel('Gradient')\n",
    "plt.title('Gradient as a function of m')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(possible_b, grad_b.mean(axis=1))\n",
    "plt.xlabel('Value of b')\n",
    "plt.ylabel('Gradient')\n",
    "plt.title('Gradient as a function of b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that at each value of $m$ or $b$ the gradient tells\n",
    "us which way will *increase* the loss. Below the optimal value of $\\hat{m}$,\n",
    "it's telling us that decreasing $\\hat{m}$ will *increase* the loss. So therefore, we\n",
    "must go in the *opposite* direction of the gradient. Let's put it all together:\n",
    "\n",
    "1. Compute the gradient for the current network parameters $\\hat{m}$ and $\\hat{b}$.\n",
    "2. Go in the opposite direction of the gradient by some fixed amount.\n",
    "3. Go back to 1.\n",
    "\n",
    "In a simple for loop, it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITER=100\n",
    "LEARNING_RATE = 0.01\n",
    "# initialize line\n",
    "line = Line()\n",
    "\n",
    "frames = []\n",
    "@gif.frame\n",
    "def plot(i):\n",
    "    y_hat = line(to_tensor(x))\n",
    "    plt.figure(dpi=300)\n",
    "    plt.title(\n",
    "        f\"Training data + network predictions\\n\"\n",
    "        f\"Learning rate is {LEARNING_RATE}, Iteration {i}\")\n",
    "\n",
    "    plt.scatter(x, y, label='Training data')\n",
    "    plt.scatter(x, to_numpy(y_hat), label='Network predictions')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.xlim([-10, 10])\n",
    "    plt.ylim([-20, 20])\n",
    "    plt.legend()\n",
    "\n",
    "for i in range(N_ITER):\n",
    "    line.zero_grad()\n",
    "    y_hat = line(to_tensor(x))\n",
    "    _loss = (y_hat - to_tensor(y)).abs().mean()\n",
    "    _loss.backward()\n",
    "    \n",
    "    for n, p in line.named_parameters():\n",
    "        p.data += - LEARNING_RATE * p.grad\n",
    "    \n",
    "    frame = plot(i)\n",
    "    frames.append(frame)\n",
    "\n",
    "show_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key hyperparameter to consider here is the learning rate. The learning rate\n",
    "controls how big of a step you take in the direction away from the gradient.\n",
    "Let's see how this parameter can affect the performance of gradient descent, by\n",
    "trying a few different values, and visualizing the learning process for each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITER=100\n",
    "LEARNING_RATES = [1.0, 0.5, 0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "line = Line()\n",
    "lines = [copy.deepcopy(line) for _ in LEARNING_RATES]\n",
    "\n",
    "frames = []\n",
    "@gif.frame\n",
    "def plot(i, lines):\n",
    "    ncols = 3\n",
    "    nrows = len(LEARNING_RATES) // ncols\n",
    "    width = ncols * 5\n",
    "    height = nrows * 4\n",
    "    fig, axs = plt.subplots(nrows, ncols, dpi=100, figsize=(width, height))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    for j, line in enumerate(lines):\n",
    "        y_hat = line(to_tensor(x))\n",
    "        axs[j].set_title(\n",
    "            f\"Training data + network predictions\\n\"\n",
    "            f\"Learning rate is {LEARNING_RATES[j]}, Iteration {i}\")\n",
    "        axs[j].scatter(x, y, label='Training data')\n",
    "        axs[j].scatter(x, to_numpy(y_hat), label='Network predictions')\n",
    "        axs[j].set_xlabel('x')\n",
    "        axs[j].set_ylabel('y')\n",
    "        axs[j].legend()\n",
    "        axs[j].set_xlim([-10, 10])\n",
    "        axs[j].set_ylim([-20, 20])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "\n",
    "for i in range(N_ITER):\n",
    "    for j, line in enumerate(lines):\n",
    "        line.zero_grad()\n",
    "        y_hat = line(to_tensor(x))\n",
    "        _loss = (y_hat - to_tensor(y)).abs().mean()\n",
    "        _loss.backward()\n",
    "\n",
    "        for n, p in line.named_parameters():\n",
    "            p.data += - LEARNING_RATES[j] * p.grad\n",
    "\n",
    "    frame = plot(i, lines)\n",
    "    frames.append(frame)\n",
    "\n",
    "show_gif(frames, width=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the learning rate is set too high, then the correct solution is never reached as the steps being taken are much\n",
    "too large. This results in the optimization oscillating back and forth between different\n",
    "parameters. At the more optimal learning rate of 0.01, the optimizations succeeds in finding the\n",
    "best parameters. At too low of a learning rate (0.001 and below), the optimization will eventually\n",
    "reach the optimal point but will be very inefficient in getting there.\n",
    "\n",
    "```{tip}\n",
    "You'll always want your learning rate to be set as high as possible, but not so high that\n",
    "optimization becomes unstable. Lower learning rates are generally \"safer\" in terms\n",
    "of reaching minima, but are more inefficient. Soon, we'll look at ways that you can \n",
    "monitor the health of your training procedure and how that help guide your choices\n",
    "for optimization hyperparameters.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signs of healthy training\n",
    "\n",
    "The network that we've looked at so far is an exceedingly simple one. Deep audio\n",
    "models are of course not single one-weight layer networks. Much of\n",
    "the analysis that we've done so far is not possible in high dimensions. There are\n",
    "essentially two core tools that one can use to diagnose and monitor network\n",
    "training:\n",
    "\n",
    "- The training and validation loss\n",
    "- The gradient norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond SGD: Momentum and friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
